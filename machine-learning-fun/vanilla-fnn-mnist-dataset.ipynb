{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist dataset\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy\n",
    "    fp = os.path.join(tempfile.gettempdir(), hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return numpy.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions_Y, train_Y):\n",
    "    predictions_count = predictions_Y.shape[0]\n",
    "    scores = predictions_Y[range(predictions_count), train_Y]\n",
    "    with np.errstate(divide='ignore'):\n",
    "        log_likelihood = -np.where(scores > 0.00001, np.log(scores), np.log(np.full(scores.shape, 0.00001)))\n",
    "        log_likelihood[np.isneginf(log_likelihood)] = -1e9\n",
    "    return np.sum(log_likelihood) / predictions_count\n",
    "\n",
    "\n",
    "class NNLayer(object):\n",
    "    def __init__(self, size, activation_func='relu'):\n",
    "        # self.W = np.random.randn(size[0], size[1]) * math.sqrt(2.0 / size[1])\n",
    "        self.W = np.random.uniform(-1.0, 1.0, size) / np.sqrt(size[0] * size[1]).astype(np.float32)\n",
    "        self.bias = np.random.random(size[0]) * 0.01\n",
    "        self.activation_func = activation_func\n",
    "        self.X = np.zeros(size[1])\n",
    "        self.Z = np.zeros(size[0])\n",
    "        self.Sigma = np.zeros(size[0])\n",
    "        self.dL_dW = np.zeros(size)\n",
    "        self.dL_dB = np.zeros(size[0])\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(self.X, self.W.T) + self.bias\n",
    "        # Each row corresponds to an activation within the layer\n",
    "        self.Sigma = self.apply_activation(self.Z)\n",
    "        return self.Sigma\n",
    "\n",
    "    def apply_activation(self, Z):\n",
    "        if self.activation_func == 'relu':\n",
    "            return np.maximum(0.0, Z)\n",
    "        elif self.activation_func == 'cross_entropy_softmax':\n",
    "            return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def backward(self, dL_dSigmas):\n",
    "        # Each row in dL_dSigmas correspond to the derivative of the loss wrt. softmax's input\n",
    "        # This is a batch operation that is done for all the neurons in the current layer\n",
    "        dZ_dX = self.W\n",
    "        # Each row in X corresponds to a unique input in the batch\n",
    "        dZ_dW = self.X\n",
    "        dZ_dB = np.ones(self.bias.shape)\n",
    "\n",
    "        if self.activation_func == 'cross_entropy_softmax':\n",
    "            # dL_dSigmas actually refers to the derivative of the loss function wrt. the softmax input which is Z\n",
    "            # So it is actually dL_dZ rather than dL_dSigmas\n",
    "            dL_dZ = dL_dSigmas\n",
    "        elif self.activation_func == 'relu':\n",
    "            dL_dZ = dL_dSigmas\n",
    "            dL_dZ[self.Z <= 0] = 0.0\n",
    "        else:\n",
    "            raise NameError('Unsupported activation function ' + self.activation_func)\n",
    "\n",
    "        # dL_dW should be a matrix: each row corresponds to the derivative of the loss function wrt. the neuron's weights\n",
    "        dL_dW = np.dot(dZ_dW.T, dL_dZ).T\n",
    "\n",
    "        # dL_dB should a vector: each component correspond to the derivative of the loss function wrt. the neuron's bias\n",
    "        dL_dB = np.sum(dL_dZ, axis=0) * dZ_dB\n",
    "\n",
    "        # dL_dX should be a matrix: each row corresponds to the derivative of the loss function wrt. layer's input\n",
    "        dL_dX = np.dot(dL_dZ, dZ_dX)\n",
    "\n",
    "        assert dL_dX.shape == self.X.shape\n",
    "        assert dL_dW.shape == self.W.shape\n",
    "        assert dL_dB.shape == self.bias.shape\n",
    "\n",
    "        self.dL_dW = dL_dW\n",
    "        self.dL_dB = dL_dB\n",
    "        return dL_dX\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.W -= self.dL_dW * learning_rate\n",
    "        self.bias -= self.dL_dB * learning_rate\n",
    "        # Reinitialize accumulated derivatives `dL_dW` and `dL_dB`\n",
    "        self.dL_dW = np.zeros(self.dL_dW.shape)\n",
    "        self.dL_dB = np.zeros(self.dL_dB.shape)\n",
    "\n",
    "    def get_neuron_gradient_vector(self, neuron_index):\n",
    "        return self.dL_dW[neuron_index]\n",
    "\n",
    "\n",
    "class VanillaFCNN(object):\n",
    "\n",
    "    def __init__(self, training_X, training_Y, input_output):\n",
    "        self.layers = []\n",
    "        self.training_X = training_X\n",
    "        self.training_Y = training_Y\n",
    "        self.input_output = input_output\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def train(self, max_epoch, learning_rate, batch_size):\n",
    "        t = trange(max_epoch)\n",
    "        loss = 0.0\n",
    "        for i in t:\n",
    "            # Create batch\n",
    "            sample = np.random.randint(0, self.training_X.shape[0], size=batch_size)\n",
    "            batch_train_X = self.training_X[sample].reshape((-1, 28 * 28))\n",
    "            batch_train_Y = self.training_Y[sample]\n",
    "\n",
    "            predictions_Y = self.forward(batch_train_X)\n",
    "\n",
    "            loss += cross_entropy_loss(predictions_Y, batch_train_Y)\n",
    "            accuracy = np.mean(np.argmax(predictions_Y, axis=1) == batch_train_Y)\n",
    "\n",
    "            # Backpropagation\n",
    "            # Start with the derivative of the Loss function wrt. the softmax function's input\n",
    "            dL_dZ = predictions_Y\n",
    "            dL_dZ[range(batch_size), batch_train_Y] -= 1\n",
    "\n",
    "            for layer in reversed(self.layers):\n",
    "                dL_dZ = layer.backward(dL_dZ)\n",
    "\n",
    "            # Update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update(learning_rate)\n",
    "\n",
    "            t.set_description(\n",
    "                'Epoch %d / %d, Loss = %f, Accuracy = %.2f' % (i + 1, max_epoch, loss / (i + 1), accuracy.mean()))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000 / 1000, Loss = 0.616597, Accuracy = 0.92: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 166.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network():\n",
    "    X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28*28) / 255.0\n",
    "    Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "    X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28, 28) / 255.0\n",
    "    Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "    inputShapeSize = 784\n",
    "    outputShapeSize = 10\n",
    "    model = VanillaFCNN(X_train, Y_train, (inputShapeSize, outputShapeSize))\n",
    "    model.addLayer(NNLayer((128, inputShapeSize), 'relu'))\n",
    "    model.addLayer(NNLayer((outputShapeSize, 128), 'cross_entropy_softmax'))\n",
    "    return model, X_test, Y_test\n",
    "\n",
    "\n",
    "network_parameters = {'delta': 0.001, 'epochs': 1000, 'batch_size': 64}\n",
    "\n",
    "model, X_test, Y_test = create_neural_network()\n",
    "model.train(network_parameters['epochs'], network_parameters['delta'], network_parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x243e7bfe5c0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate our model to check which number it thinks this image represents\n",
    "model.predict(X_test[6]).argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
