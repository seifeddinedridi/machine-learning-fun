{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist dataset\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy\n",
    "    fp = os.path.join(tempfile.gettempdir(), hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return numpy.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions_Y, train_Y):\n",
    "    predictions_count = predictions_Y.shape[0]\n",
    "    scores = predictions_Y[range(predictions_count), train_Y]\n",
    "    with np.errstate(divide='ignore'):\n",
    "        log_likelihood = -np.where(scores > 0.00001, np.log(scores), np.log(np.full(scores.shape, 0.00001)))\n",
    "        log_likelihood[np.isneginf(log_likelihood)] = -1e9\n",
    "    return np.sum(log_likelihood) / predictions_count\n",
    "\n",
    "\n",
    "class NNLayer(object):\n",
    "    def __init__(self, size, activation_func='relu'):\n",
    "        # self.W = np.random.randn(size[0], size[1]) * math.sqrt(2.0 / size[1])\n",
    "        self.W = np.random.uniform(-1.0, 1.0, size) / np.sqrt(size[0] * size[1]).astype(np.float32)\n",
    "        self.bias = np.random.random(size[0]) * 0.01\n",
    "        self.activation_func = activation_func\n",
    "        self.X = None\n",
    "        self.Z = None\n",
    "        self.Sigma = None\n",
    "        self.dL_dW = None\n",
    "        self.dL_dB = None\n",
    "        self.delta_W = np.zeros(size) # Used to compute analytical gradient\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(self.X, (self.W + self.delta_W).T) + self.bias\n",
    "        # Each row corresponds to an activation within the layer\n",
    "        self.Sigma = self.apply_activation(self.Z)\n",
    "        return self.Sigma\n",
    "\n",
    "    def apply_activation(self, Z):\n",
    "        if self.activation_func == 'relu':\n",
    "            return np.maximum(0.0, Z)\n",
    "        elif self.activation_func == 'cross_entropy_softmax':\n",
    "            return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def backward(self, dL_dSigmas):\n",
    "        # Each row in dL_dSigmas correspond to the derivative of the loss wrt. softmax's input\n",
    "        # This is a batch operation that is done for all the neurons in the current layer\n",
    "        dZ_dX = self.W + self.delta_W\n",
    "        # Each row in X corresponds to a unique input in the batch\n",
    "        dZ_dW = self.X\n",
    "        dZ_dB = np.ones(self.bias.shape)\n",
    "\n",
    "        if self.activation_func == 'cross_entropy_softmax':\n",
    "            # dL_dSigmas actually refers to the derivative of the loss function wrt. the softmax input which is Z\n",
    "            # So it is actually dL_dZ rather than dL_dSigmas\n",
    "            dL_dZ = dL_dSigmas\n",
    "        elif self.activation_func == 'relu':\n",
    "            dL_dZ = dL_dSigmas\n",
    "            dL_dZ[self.Z <= 0] = 0.0\n",
    "        else:\n",
    "            raise NameError('Unsupported activation function ' + self.activation_func)\n",
    "\n",
    "        # dL_dW should be a matrix: each row corresponds to the derivative of the loss function wrt. the neuron's weights\n",
    "        dL_dW = np.dot(dZ_dW.T, dL_dZ).T\n",
    "\n",
    "        # dL_dB should a vector: each component correspond to the derivative of the loss function wrt. the neuron's bias\n",
    "        dL_dB = np.sum(dL_dZ, axis=0) * dZ_dB\n",
    "\n",
    "        # dL_dX should be a matrix: each row corresponds to the derivative of the loss function wrt. layer's input\n",
    "        dL_dX = np.dot(dL_dZ, dZ_dX)\n",
    "\n",
    "        assert dL_dX.shape == self.X.shape\n",
    "        assert dL_dW.shape == self.W.shape\n",
    "        assert dL_dB.shape == self.bias.shape\n",
    "\n",
    "        self.dL_dW = dL_dW\n",
    "        self.dL_dB = dL_dB\n",
    "        return dL_dX\n",
    "\n",
    "    def update(self, learning_rate, keep_gradients=False):\n",
    "        self.W -= self.dL_dW * learning_rate\n",
    "        self.bias -= self.dL_dB * learning_rate\n",
    "        if keep_gradients == False:\n",
    "            # Reinitialize accumulated derivatives `dL_dW` and `dL_dB`\n",
    "            self.dL_dW = np.zeros(self.dL_dW.shape)\n",
    "            self.dL_dB = np.zeros(self.dL_dB.shape)\n",
    "\n",
    "    def set_delta_weights(self, neuron_index, input_index, delta):\n",
    "        self.delta_W[neuron_index, input_index] = delta\n",
    "\n",
    "class VanillaFCNN(object):\n",
    "\n",
    "    def __init__(self, training_X, training_Y, input_output, keep_gradients=False):\n",
    "        self.layers = []\n",
    "        self.training_X = training_X\n",
    "        self.training_Y = training_Y\n",
    "        self.input_output = input_output\n",
    "        self.keep_gradients = keep_gradients\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def train(self, max_epoch, learning_rate, batch_size):\n",
    "        t = trange(max_epoch)\n",
    "        loss = 0.0\n",
    "        for i in t:\n",
    "            # Create batch\n",
    "            sample = np.random.randint(0, self.training_X.shape[0], size=batch_size)\n",
    "            batch_train_X = self.training_X[sample].reshape((-1, 28 * 28))\n",
    "            batch_train_Y = self.training_Y[sample]\n",
    "\n",
    "            predictions_Y = self.forward(batch_train_X)\n",
    "\n",
    "            loss += cross_entropy_loss(predictions_Y, batch_train_Y)\n",
    "            accuracy = np.mean(np.argmax(predictions_Y, axis=1) == batch_train_Y)\n",
    "\n",
    "            # Backpropagation\n",
    "            # Start with the derivative of the Loss function wrt. the softmax function's input\n",
    "            dL_dZ = predictions_Y\n",
    "            dL_dZ[range(batch_size), batch_train_Y] -= 1\n",
    "\n",
    "            for layer in reversed(self.layers):\n",
    "                dL_dZ = layer.backward(dL_dZ)\n",
    "    \n",
    "             # Update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update(learning_rate, self.keep_gradients)\n",
    "\n",
    "            t.set_description(\n",
    "                'Epoch %d / %d, Loss = %f, Accuracy = %.2f' % (i + 1, max_epoch, loss / (i + 1), accuracy.mean()))\n",
    "\n",
    "    def __call__(self, input_index, learning_rate):\n",
    "        batch_train_X = self.training_X[input_index].reshape((-1, 28 * 28))\n",
    "        batch_train_Y = self.training_Y[input_index]\n",
    "        \n",
    "        predictions_Y = self.forward(batch_train_X)\n",
    "\n",
    "        dL_dZ = predictions_Y\n",
    "        dL_dZ[range(1), batch_train_Y] -= 1\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dZ = layer.backward(dL_dZ)\n",
    "        \n",
    "        # Update the weights\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate, self.keep_gradients)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.forward(X.reshape(1, -1))\n",
    "\n",
    "    def get_layer_gradient_matrix(self, layer_index):\n",
    "        return self.layers[layer_index].dL_dW\n",
    "\n",
    "    def compute_analytical_gradient(self, input_index, delta):\n",
    "        sample_X = self.training_X[input_index].reshape(1, -1)\n",
    "        sample_Y = self.training_Y[input_index]\n",
    "        \n",
    "        # Evaluate base loss\n",
    "        predictions_Y = self.forward(sample_X)\n",
    "        base_loss = cross_entropy_loss(predictions_Y, sample_Y)\n",
    "        gradient = []\n",
    "        \n",
    "        for layer_index, layer in enumerate(self.layers):\n",
    "            layer_neurons_count = layer.W.shape[0]\n",
    "            layer_input_size = layer.W.shape[1]\n",
    "            \n",
    "            layer_gradients = np.zeros((layer_neurons_count, layer_input_size))\n",
    "            for neuron_index in range(layer_neurons_count):\n",
    "                for input_index in range(layer_input_size):\n",
    "                    layer.set_delta_weights(neuron_index, input_index, delta)\n",
    "            \n",
    "                    predictions_Y = self.forward(sample_X)\n",
    "                    loss = cross_entropy_loss(predictions_Y, sample_Y)\n",
    "            \n",
    "                    layer_gradients[neuron_index, input_index] = (loss - base_loss) / delta\n",
    "\n",
    "                    # Reset delta weights\n",
    "                    layer.set_delta_weights(neuron_index, input_index, 0.0)\n",
    "                    \n",
    "            gradient.append(layer_gradients)\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000 / 1000, Loss = 0.413388, Accuracy = 0.97: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:09<00:00, 103.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network():\n",
    "    X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28*28) / 255.0\n",
    "    Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "    X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28, 28) / 255.0\n",
    "    Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "    inputShapeSize = 784\n",
    "    outputShapeSize = 10\n",
    "    model = VanillaFCNN(training_X=X_train, training_Y=Y_train, input_output=(inputShapeSize, outputShapeSize), keep_gradients=True)\n",
    "    model.addLayer(NNLayer((128, inputShapeSize), 'relu'))\n",
    "    model.addLayer(NNLayer((outputShapeSize, 128), 'cross_entropy_softmax'))\n",
    "    return model, X_test, Y_test\n",
    "\n",
    "\n",
    "network_parameters = {'delta': 0.01, 'epochs': 1000, 'batch_size': 128}\n",
    "\n",
    "model, X_test, Y_test = create_neural_network()\n",
    "model.train(network_parameters['epochs'], network_parameters['delta'], network_parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17882c92588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate our model to check which number it thinks this image represents\n",
    "model.predict(X_test[100]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = 0\n",
    "# Gradient check\n",
    "gradient_matrix = model.compute_analytical_gradient(input_index=0, delta=network_parameters['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03808563977963624\n",
      "0.04166770941846269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17882e3e1d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_index = 1\n",
    "print(gradient_matrix[layer_index].min())\n",
    "print(gradient_matrix[layer_index].max())\n",
    "plt.imshow(gradient_matrix[layer_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.040897480918649705\n",
      "0.040699497000275045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17882dd5e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model and do backward propagation on a single input\n",
    "model(input_index=0, learning_rate=network_parameters['delta'])\n",
    "\n",
    "layer_gradients = model.get_layer_gradient_matrix(layer_index)\n",
    "print(layer_gradients.min())\n",
    "print(layer_gradients.max())\n",
    "plt.imshow(layer_gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}