{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist dataset\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy\n",
    "    fp = os.path.join(tempfile.gettempdir(), hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return numpy.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions_Y, train_Y):\n",
    "    predictions_count = predictions_Y.shape[0]\n",
    "    scores = predictions_Y[range(predictions_count), train_Y]\n",
    "    with np.errstate(divide='ignore'):\n",
    "        log_likelihood = -np.where(scores > 0.00001, np.log(scores), np.log(np.full(scores.shape, 0.00001)))\n",
    "        log_likelihood[np.isneginf(log_likelihood)] = -1e9\n",
    "    return np.sum(log_likelihood) / predictions_count\n",
    "\n",
    "\n",
    "class NNLayer(object):\n",
    "    def __init__(self, size, activation_func='relu'):\n",
    "        # self.W = np.random.randn(size[0], size[1]) * math.sqrt(2.0 / size[1])\n",
    "        self.W = np.random.uniform(-1.0, 1.0, size) / np.sqrt(size[0] * size[1]).astype(np.float32)\n",
    "        self.bias = np.random.random(size[0]) * 0.01\n",
    "        self.activation_func = activation_func\n",
    "        self.X = np.zeros(size[1])\n",
    "        self.Z = np.zeros(size[0])\n",
    "        self.Sigma = np.zeros(size[0])\n",
    "        self.dL_dW = np.zeros(size)\n",
    "        self.dL_dB = np.zeros(size[0])\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(self.X, self.W.T) + self.bias\n",
    "        # Each row corresponds to an activation within the layer\n",
    "        self.Sigma = self.apply_activation(self.Z)\n",
    "        return self.Sigma\n",
    "\n",
    "    def apply_activation(self, Z):\n",
    "        if self.activation_func == 'relu':\n",
    "            return np.maximum(0.0, Z)\n",
    "        elif self.activation_func == 'cross_entropy_softmax':\n",
    "            return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def backward(self, dL_dSigmas):\n",
    "        # Each row in dL_dSigmas correspond to the derivative of the loss wrt. softmax's input\n",
    "        # This is a batch operation that is done for all the neurons in the current layer\n",
    "        dZ_dX = self.W\n",
    "        # Each row in X corresponds to a unique input in the batch\n",
    "        dZ_dW = self.X\n",
    "        dZ_dB = np.ones(self.bias.shape)\n",
    "\n",
    "        if self.activation_func == 'cross_entropy_softmax':\n",
    "            # dL_dSigmas actually refers to the derivative of the loss function wrt. the softmax input which is Z\n",
    "            # So it is actually dL_dZ rather than dL_dSigmas\n",
    "            dL_dZ = dL_dSigmas\n",
    "        elif self.activation_func == 'relu':\n",
    "            dL_dZ = dL_dSigmas\n",
    "            dL_dZ[self.Z <= 0] = 0.0\n",
    "        else:\n",
    "            raise NameError('Unsupported activation function ' + self.activation_func)\n",
    "\n",
    "        # dL_dW should be a matrix: each row corresponds to the derivative of the loss function wrt. the neuron's weights\n",
    "        dL_dW = np.dot(dZ_dW.T, dL_dZ).T\n",
    "\n",
    "        # dL_dB should a vector: each component correspond to the derivative of the loss function wrt. the neuron's bias\n",
    "        dL_dB = np.sum(dL_dZ, axis=0) * dZ_dB\n",
    "\n",
    "        # dL_dX should be a matrix: each row corresponds to the derivative of the loss function wrt. layer's input\n",
    "        dL_dX = np.dot(dL_dZ, dZ_dX)\n",
    "\n",
    "        assert dL_dX.shape == self.X.shape\n",
    "        assert dL_dW.shape == self.W.shape\n",
    "        assert dL_dB.shape == self.bias.shape\n",
    "\n",
    "        self.dL_dW = dL_dW\n",
    "        self.dL_dB = dL_dB\n",
    "        return dL_dX\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.W -= self.dL_dW * learning_rate\n",
    "        self.bias -= self.dL_dB * learning_rate\n",
    "        # Reinitialize accumulated derivatives `dL_dW` and `dL_dB`\n",
    "        self.dL_dW = np.zeros(self.dL_dW.shape)\n",
    "        self.dL_dB = np.zeros(self.dL_dB.shape)\n",
    "\n",
    "    def get_neuron_gradient_vector(self, neuron_index):\n",
    "        return self.dL_dW[neuron_index]\n",
    "\n",
    "\n",
    "class VanillaFCNN(object):\n",
    "\n",
    "    def __init__(self, training_X, training_Y, input_output):\n",
    "        self.layers = []\n",
    "        self.training_X = training_X\n",
    "        self.training_Y = training_Y\n",
    "        self.input_output = input_output\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def train(self, max_epoch, learning_rate, batch_size):\n",
    "        t = trange(max_epoch)\n",
    "        loss = 0.0\n",
    "        for i in t:\n",
    "            # Create batch\n",
    "            sample = np.random.randint(0, self.training_X.shape[0], size=batch_size)\n",
    "            batch_train_X = self.training_X[sample].reshape((-1, 28 * 28))\n",
    "            batch_train_Y = self.training_Y[sample]\n",
    "\n",
    "            predictions_Y = self.forward(batch_train_X)\n",
    "\n",
    "            loss += cross_entropy_loss(predictions_Y, batch_train_Y)\n",
    "            accuracy = np.mean(np.argmax(predictions_Y, axis=1) == batch_train_Y)\n",
    "\n",
    "            # Backpropagation\n",
    "            # Start with the derivative of the Loss function wrt. the softmax function's input\n",
    "            dL_dZ = predictions_Y\n",
    "            dL_dZ[range(batch_size), batch_train_Y] -= 1\n",
    "\n",
    "            for layer in reversed(self.layers):\n",
    "                dL_dZ = layer.backward(dL_dZ)\n",
    "\n",
    "            # Update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update(learning_rate)\n",
    "\n",
    "            t.set_description(\n",
    "                'Epoch %d / %d, Loss = %f, Accuracy = %.2f' % (i + 1, max_epoch, loss / (i + 1), accuracy.mean()))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000 / 1000, Loss = 0.259918, Accuracy = 1.00: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 160.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network():\n",
    "    X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28*28) / 255.0\n",
    "    Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "    X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape(-1, 28, 28) / 255.0\n",
    "    Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "    inputShapeSize = 784\n",
    "    outputShapeSize = 10\n",
    "    model = VanillaFCNN(X_train, Y_train, (inputShapeSize, outputShapeSize))\n",
    "    model.addLayer(NNLayer((128, inputShapeSize), 'relu'))\n",
    "    model.addLayer(NNLayer((outputShapeSize, 128), 'cross_entropy_softmax'))\n",
    "    return model, X_test, Y_test\n",
    "\n",
    "\n",
    "network_parameters = {'delta': 0.01, 'epochs': 1000, 'batch_size': 64}\n",
    "\n",
    "model, X_test, Y_test = create_neural_network()\n",
    "model.train(network_parameters['epochs'], network_parameters['delta'], network_parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x257d6478ef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate our model to check which number it thinks this image represents\n",
    "model.predict(X_test[100]).argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
